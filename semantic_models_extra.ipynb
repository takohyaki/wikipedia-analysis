{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qns_answers = pd.read_csv('data/updated_qns_answers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>points</th>\n",
       "      <th>article</th>\n",
       "      <th>nlp_analysis</th>\n",
       "      <th>readability_score</th>\n",
       "      <th>preprocessed_question</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>article_text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>did the people of gibraltar vote to remain pa...</td>\n",
       "      <td>58</td>\n",
       "      <td>Gibraltar</td>\n",
       "      <td>([('the united kingdom', 'GPE'), ('2002', 'DAT...</td>\n",
       "      <td>62.68</td>\n",
       "      <td>people gibraltar vote remain part united kingd...</td>\n",
       "      <td>15222</td>\n",
       "      <td>https://simple.wikipedia.org/wiki/Gibraltar</td>\n",
       "      <td>Gibraltar is an Overseas Territory of the Unit...</td>\n",
       "      <td>gibraltar overseas territory united kingdom me...</td>\n",
       "      <td>192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>which country uses the franc as its official ...</td>\n",
       "      <td>55</td>\n",
       "      <td>Currency</td>\n",
       "      <td>([], [(' ', 'dep', 'which'), ('which', 'det', ...</td>\n",
       "      <td>53.88</td>\n",
       "      <td>country us franc official currency</td>\n",
       "      <td>2140</td>\n",
       "      <td>https://simple.wikipedia.org/wiki/Currency</td>\n",
       "      <td>Currency is the unit of money used by the peop...</td>\n",
       "      <td>currency unit money used people country union ...</td>\n",
       "      <td>127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>which of these old communist parties no longe...</td>\n",
       "      <td>52</td>\n",
       "      <td>List of communist parties</td>\n",
       "      <td>([('communist', 'NORP'), ('today', 'DATE')], [...</td>\n",
       "      <td>69.79</td>\n",
       "      <td>old communist party longer exists today</td>\n",
       "      <td>4402</td>\n",
       "      <td>https://simple.wikipedia.org/wiki/List%20of%20...</td>\n",
       "      <td>There are a number of communist parties around...</td>\n",
       "      <td>number communist party around world world hist...</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a patient has a terminal illness and wants to ...</td>\n",
       "      <td>65</td>\n",
       "      <td>Medical ethics</td>\n",
       "      <td>([], [('a', 'det', 'patient'), ('patient', 'ns...</td>\n",
       "      <td>66.23</td>\n",
       "      <td>patient terminal illness want end life family ...</td>\n",
       "      <td>13938</td>\n",
       "      <td>https://simple.wikipedia.org/wiki/Medical%20et...</td>\n",
       "      <td>Medical ethics is the set of ethical rules tha...</td>\n",
       "      <td>medical ethic set ethical rule doctor follow i...</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>according to plato, what are the three types o...</td>\n",
       "      <td>55</td>\n",
       "      <td>The Republic</td>\n",
       "      <td>([('three', 'CARDINAL')], [('according', 'prep...</td>\n",
       "      <td>71.14</td>\n",
       "      <td>according plato three type people society made</td>\n",
       "      <td>13148</td>\n",
       "      <td>https://simple.wikipedia.org/wiki/The%20Republic</td>\n",
       "      <td>The Republic is a book by Plato. It was finish...</td>\n",
       "      <td>republic book plato finished bc asks question ...</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  points  \\\n",
       "0   did the people of gibraltar vote to remain pa...      58   \n",
       "1   which country uses the franc as its official ...      55   \n",
       "2   which of these old communist parties no longe...      52   \n",
       "3  a patient has a terminal illness and wants to ...      65   \n",
       "4  according to plato, what are the three types o...      55   \n",
       "\n",
       "                     article  \\\n",
       "0                  Gibraltar   \n",
       "1                   Currency   \n",
       "2  List of communist parties   \n",
       "3             Medical ethics   \n",
       "4               The Republic   \n",
       "\n",
       "                                        nlp_analysis  readability_score  \\\n",
       "0  ([('the united kingdom', 'GPE'), ('2002', 'DAT...              62.68   \n",
       "1  ([], [(' ', 'dep', 'which'), ('which', 'det', ...              53.88   \n",
       "2  ([('communist', 'NORP'), ('today', 'DATE')], [...              69.79   \n",
       "3  ([], [('a', 'det', 'patient'), ('patient', 'ns...              66.23   \n",
       "4  ([('three', 'CARDINAL')], [('according', 'prep...              71.14   \n",
       "\n",
       "                               preprocessed_question     id  \\\n",
       "0  people gibraltar vote remain part united kingd...  15222   \n",
       "1                 country us franc official currency   2140   \n",
       "2            old communist party longer exists today   4402   \n",
       "3  patient terminal illness want end life family ...  13938   \n",
       "4     according plato three type people society made  13148   \n",
       "\n",
       "                                                 url  \\\n",
       "0        https://simple.wikipedia.org/wiki/Gibraltar   \n",
       "1         https://simple.wikipedia.org/wiki/Currency   \n",
       "2  https://simple.wikipedia.org/wiki/List%20of%20...   \n",
       "3  https://simple.wikipedia.org/wiki/Medical%20et...   \n",
       "4   https://simple.wikipedia.org/wiki/The%20Republic   \n",
       "\n",
       "                                        article_text  \\\n",
       "0  Gibraltar is an Overseas Territory of the Unit...   \n",
       "1  Currency is the unit of money used by the peop...   \n",
       "2  There are a number of communist parties around...   \n",
       "3  Medical ethics is the set of ethical rules tha...   \n",
       "4  The Republic is a book by Plato. It was finish...   \n",
       "\n",
       "                                   preprocessed_text  cluster  \n",
       "0  gibraltar overseas territory united kingdom me...    192.0  \n",
       "1  currency unit money used people country union ...    127.0  \n",
       "2  number communist party around world world hist...    121.0  \n",
       "3  medical ethic set ethical rule doctor follow i...    117.0  \n",
       "4  republic book plato finished bc asks question ...    117.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_matching_dataset = qns_answers.copy()\n",
    "\n",
    "semantic_matching_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tammiekoh/Documents/projects/wikipedia-analysis/.venv/lib/python3.9/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "# Load SpaCy model\n",
    "nlp = spacy.load('en_core_web_md', disable=['parser', 'tagger', 'ner'])\n",
    "\n",
    "def get_spacy_vectors(texts, batch_size=100):\n",
    "    vectors = []\n",
    "    for doc in nlp.pipe(texts, batch_size=batch_size):\n",
    "        vectors.append(doc.vector)\n",
    "    return np.array(vectors)\n",
    "\n",
    "# Ensure all text data is string and replace np.nan values with empty strings\n",
    "semantic_matching_dataset['preprocessed_question'] = semantic_matching_dataset['preprocessed_question'].fillna('').astype(str)\n",
    "semantic_matching_dataset['preprocessed_text'] = semantic_matching_dataset['preprocessed_text'].fillna('').astype(str)\n",
    "\n",
    "# Aggregate the preprocessed text of articles by their cluster to create a single document per cluster\n",
    "cluster_documents = semantic_matching_dataset.groupby('cluster')['preprocessed_text'].apply(lambda texts: ' '.join(texts)).reset_index(name='cluster_document')\n",
    "\n",
    "# Merge the aggregated cluster document back into the main DataFrame\n",
    "semantic_matching_dataset = pd.merge(semantic_matching_dataset, cluster_documents, on='cluster', how='left')\n",
    "\n",
    "# Generate SpaCy vectors for questions\n",
    "questions = semantic_matching_dataset['preprocessed_question'].tolist()\n",
    "question_vectors_spacy = get_spacy_vectors(questions)\n",
    "\n",
    "# Generate SpaCy vectors for the corresponding cluster documents of each question\n",
    "# Ensure the order of cluster documents matches the order of questions\n",
    "cluster_document_vectors_spacy = get_spacy_vectors(semantic_matching_dataset['cluster_document'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate cosine similarities between each question vector and its corresponding cluster document vector\n",
    "cosine_similarities_spacy = cosine_similarity(question_vectors_spacy, cluster_document_vectors_spacy)\n",
    "\n",
    "# Since each question is compared with its corresponding cluster document, we can directly use the diagonal of the similarity matrix\n",
    "cosine_sim_scores = np.diag(cosine_similarities_spacy)\n",
    "semantic_matching_dataset['cosine_sim_with_cluster_spacy'] = cosine_sim_scores\n",
    "\n",
    "# Now, semantic_matching_dataset contains a 'cosine_sim_with_cluster_spacy' column with the cosine similarity of each question to its associated cluster's aggregated content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_matching_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can explore further by finding which cluster has the highest cosine similarity for every question to refine the clustering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "def encode_texts_in_batches(texts, model, batch_size=128):\n",
    "    vectors = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        # ensure the model's output is converted to tensors and moved to CPU before converting to numpy\n",
    "        batch_vectors = model.encode(batch, convert_to_tensor=True).cpu().numpy()\n",
    "        vectors.extend(batch_vectors)\n",
    "    return np.array(vectors)\n",
    "\n",
    "question_vectors_sbert = encode_texts_in_batches(\n",
    "    semantic_matching_dataset['question'].tolist(), sbert_model)\n",
    "article_vectors_sbert = encode_texts_in_batches(\n",
    "    semantic_matching_dataset['text'].tolist(), sbert_model)\n",
    "\n",
    "cosine_sim_scores = [cosine_similarity([q_vec], [a_vec])[0][0] for q_vec, a_vec in zip(question_vectors_sbert, article_vectors_sbert)]\n",
    "semantic_matching_dataset['cosine_sim_sbert'] = cosine_sim_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may need to delete cached models before running use model\n",
    "# !rm -rf /var/folders/j3/2xy_ffxd6jq618phh49pk1b00000gn/T/tfhub_modules/*\n",
    "\n",
    "use_model = hub.load('https://tfhub.dev/google/universal-sentence-encoder/4')\n",
    "\n",
    "\n",
    "def batch_get_use_vector(texts, batch_size=32):\n",
    "    vectors = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_vectors = use_model(batch_texts).numpy()\n",
    "        vectors.append(batch_vectors)\n",
    "    return np.vstack(vectors)\n",
    "\n",
    "questions = semantic_matching_dataset['question'].tolist()\n",
    "articles = semantic_matching_dataset['text'].tolist()\n",
    "\n",
    "semantic_matching_dataset['question_vector_use'] = list(map(list, batch_get_use_vector(questions)))\n",
    "semantic_matching_dataset['article_vector_use'] = list(map(list, batch_get_use_vector(articles)))\n",
    "\n",
    "cosine_sims = np.array([\n",
    "    cosine_similarity([q_vec], [a_vec])[0][0] \n",
    "    for q_vec, a_vec in zip(semantic_matching_dataset['question_vector_use'], semantic_matching_dataset['article_vector_use'])\n",
    "])\n",
    "\n",
    "semantic_matching_dataset['cosine_sim_use'] = cosine_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_matching_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sim_spacy = np.mean(semantic_matching_dataset['cosine_sim_spacy'])\n",
    "mean_sim_sbert = np.mean(semantic_matching_dataset['cosine_sim_sbert'])\n",
    "mean_sim_use = np.mean(semantic_matching_dataset['cosine_sim_use'])\n",
    "\n",
    "print(f\"Mean Cosine Similarity (SpaCy): {mean_sim_spacy}\")\n",
    "print(f\"Mean Cosine Similarity (SBERT): {mean_sim_sbert}\")\n",
    "print(f\"Mean Cosine Similarity (USE): {mean_sim_use}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy embedding model clearly outperforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(semantic_matching_dataset['points'], semantic_matching_dataset['cosine_sim_spacy'], label='SpaCy', alpha=0.5)\n",
    "plt.scatter(semantic_matching_dataset['points'], semantic_matching_dataset['cosine_sim_sbert'], label='SBERT', alpha=0.5)\n",
    "plt.scatter(semantic_matching_dataset['points'], semantic_matching_dataset['cosine_sim_use'], label='USE', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Points')\n",
    "plt.ylabel('Cosine Similarity')\n",
    "plt.title('Correlation between Points and Cosine Similarity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seems that spacy model captures a good portion of the lower scored questions. let's explore the question-article pairs that scored a near perfect 1.00 similarity score, as well as the opposite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_matching_dataset[semantic_matching_dataset['cosine_sim_spacy'] >= 0.9]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direct Question-Answer Relation: Each question directly relates to the topic of the article. The keywords from the questions are explicitly present in the text of the articles, leading to a higher similarity score.\n",
    "\n",
    "Specificity: The questions are specific, and the articles contain detailed information that directly answers these questions. This specificity likely results in a higher concentration of relevant terms and less noise, which boosts similarity scores.\n",
    "\n",
    "Relevance of Content: The content of the questions closely aligns with the main subject of the articles. For example, a question about the environment directly relates to an article titled \"Environment,\" and a question about MRI accuracy directly relates to an article about \"Magnetic resonance imaging.\"\n",
    "\n",
    "High readability scores as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_matching_dataset[semantic_matching_dataset['cosine_sim_spacy'] <= 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mismatch of Context: The questions are about specific events or people, while the articles titled with years likely cover a wide range of events that happened in that year. The spaCy embeddings might not find a strong contextual match between the question's focus and the broad content of the article.\n",
    "\n",
    "Lack of Specificity in Articles: Articles that are simply titled with a year might not have a strong thematic focus, leading to a diluted set of vectors that don't match well with the more focused vectors generated from the questions.\n",
    "\n",
    "Semantic Ambiguity: Years by themselves do not carry specific semantic information. Without additional context, the model may struggle to link a question about a specific event or individual to an article that broadly covers everything related to that year.\n",
    "\n",
    "Named Entity Recognition (NER) Challenges: If the model is heavily weighing named entities, such as specific names and dates, it may not correctly associate the relevance of the year to the specific event or individual in the question.\n",
    "\n",
    "Temporal Relevance: Questions that ask about a particular date or event may not align well with the content of an article that summarizes an entire year. The temporal focus is too broad in the article to match the specificity of the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given that we are not able to modify the training and test data set, fine tuning the data for the spacy model would hence be quite challenging. we could instead look for alternative models that are specifically designed for chatbots (most common use case of question-answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the TF-IDF transformation for articles is performed\n",
    "tfidf_matrix_articles_individual = tfidf_vectorizer.transform(qns_answers['preprocessed_text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
