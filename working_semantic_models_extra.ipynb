{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 1: supervised into unsupervised\n",
    "\n",
    "using random forest to categorise into broad categories and then using unsupervised to further refine into subcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# mlb = MultiLabelBinarizer()\n",
    "# y = mlb.fit_transform(df_valid_articles['categories'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_valid_articles = pd.merge(wikipedia_articles, df_valid_articles, on='title', how='inner')\n",
    "\n",
    "# print(df_valid_articles.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# # fix this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# tfidf = TfidfVectorizer(max_features=1000)\n",
    "# X = tfidf.fit_transform(df_valid_articles['preprocessed_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multi-label classification, we will try random forest and logistic reg\n",
    "\n",
    "Random Forest with a OneVsRest strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.multioutput import MultiOutputClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# model = MultiOutputClassifier(RandomForestClassifier(), n_jobs=-1)\n",
    "# model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import hamming_loss, accuracy_score\n",
    "\n",
    "# predictions = model.predict(X_test)\n",
    "# print(f\"Hamming Loss: {hamming_loss(y_test, predictions)}\")\n",
    "# # For exact match, which might be strict for multi-label classification\n",
    "# print(f\"Exact Match Ratio: {accuracy_score(y_test, predictions)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low Hamming Loss indicates that individual label predictions are generally accurate, but the low Exact Match Ratio shows that fully correct multi-label predictions for each sample are rare. This can happen in complex multi-label tasks where getting every label right is difficult due to the intricate relationships between labels.\n",
    "The discrepancy between the two metrics highlights the challenge of multi-label classification: it's easier to predict some labels correctly than to predict all labels correctly for a given sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load a pre-trained sentence transformer model\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# # Generate embeddings\n",
    "# embeddings = model.encode(df_valid_articles['preprocessed_text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(embeddings, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# from sklearn.multioutput import MultiOutputClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# model = MultiOutputClassifier(RandomForestClassifier(), n_jobs=-1)\n",
    "# model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import hamming_loss, accuracy_score\n",
    "\n",
    "# predictions = model.predict(X_test)\n",
    "# print(f\"Hamming Loss: {hamming_loss(y_test, predictions)}\")\n",
    "# # For exact match, which might be strict for multi-label classification\n",
    "# print(f\"Exact Match Ratio: {accuracy_score(y_test, predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisit Metric Choice: Since EMR is strict (all labels must be correct), consider focusing also on other metrics like F1 score (balance between precision and recall), which might give a more nuanced view of model performance across labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: Only unsupervised\n",
    "\n",
    "uses both the categories and the text to cluster into distinct categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df_valid_articles['categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the inner join\n",
    "df_valid_articles = pd.merge(qns_answers, df_valid_articles, left_on='article', right_on='title', how='inner')\n",
    "\n",
    "# Display the first few rows of the merged DataFrame to verify the join\n",
    "print(df_valid_articles.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "X = tfidf.fit_transform(df_valid_articles['preprocessed_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# Assuming X is the TF-IDF matrix and y_encoded is the category matrix\n",
    "X_combined = hstack([X, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Decide on the number of clusters\n",
    "n_clusters = 5  # Example, adjust based on your needs\n",
    "\n",
    "# Perform clustering\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Range of n_clusters values to evaluate\n",
    "n_clusters_range = range(5, 15)\n",
    "\n",
    "silhouette_scores = []\n",
    "\n",
    "for n_clusters in n_clusters_range:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(X_combined)\n",
    "    score = silhouette_score(X_combined, clusters)\n",
    "    silhouette_scores.append(score)\n",
    "    print(f\"Silhouette Score with n_clusters = {n_clusters}: {score}\")\n",
    "\n",
    "# Plotting the Silhouette Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_clusters_range, silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Scores for Different Numbers of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()\n",
    "\n",
    "# Choose the n_clusters with the highest silhouette score as optimal\n",
    "optimal_n_clusters_silhouette = n_clusters_range[silhouette_scores.index(max(silhouette_scores))]\n",
    "print(f\"Optimal number of clusters based on silhouette score: {optimal_n_clusters_silhouette}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways we can refine this. We can start with dimensionality reduction, before proceeding to word embeddings to capture more semantic content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "# Dimensionality reduction for efficiency and to enhance clustering performance\n",
    "svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "X_reduced = svd.fit_transform(X_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(df_valid_articles['preprocessed_text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to compute KMeans clustering and silhouette scores\n",
    "def compute_silhouette_scores(X, n_clusters_range):\n",
    "    silhouette_scores = []\n",
    "    for n_clusters in n_clusters_range:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        clusters = kmeans.fit_predict(X)\n",
    "        score = silhouette_score(X, clusters)\n",
    "        silhouette_scores.append(score)\n",
    "    return silhouette_scores\n",
    "\n",
    "# Define the range of n_clusters\n",
    "n_clusters_range = np.arange(10, 101, 5)\n",
    "\n",
    "# Assuming X_combined, X_reduced, and embeddings are defined\n",
    "# Compute silhouette scores for each model\n",
    "silhouette_scores_combined = compute_silhouette_scores(X_combined, n_clusters_range)\n",
    "silhouette_scores_reduced = compute_silhouette_scores(X_reduced, n_clusters_range)\n",
    "silhouette_scores_embeddings = compute_silhouette_scores(embeddings, n_clusters_range)\n",
    "\n",
    "# Plotting the Silhouette Scores\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.plot(n_clusters_range, silhouette_scores_combined, marker='o', label='X_combined')\n",
    "plt.plot(n_clusters_range, silhouette_scores_reduced, marker='x', label='X_reduced')\n",
    "plt.plot(n_clusters_range, silhouette_scores_embeddings, marker='^', label='Embeddings')\n",
    "\n",
    "plt.title('Silhouette Scores for Different Models')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Determine the optimal number of clusters for each model\n",
    "optimal_n_clusters_combined = n_clusters_range[np.argmax(silhouette_scores_combined)]\n",
    "optimal_n_clusters_reduced = n_clusters_range[np.argmax(silhouette_scores_reduced)]\n",
    "optimal_n_clusters_embeddings = n_clusters_range[np.argmax(silhouette_scores_embeddings)]\n",
    "\n",
    "print(f\"Optimal number of clusters for X_combined: {optimal_n_clusters_combined}\")\n",
    "print(f\"Optimal number of clusters for X_reduced: {optimal_n_clusters_reduced}\")\n",
    "print(f\"Optimal number of clusters for embeddings: {optimal_n_clusters_embeddings}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering with the optimal number of clusters\n",
    "kmeans_optimal = KMeans(n_clusters=optimal_n_clusters_reduced, random_state=42)\n",
    "df_valid_articles['cluster'] = kmeans_optimal.fit_predict(X_reduced)\n",
    "\n",
    "# Qualitative Analysis of the Clustered Articles\n",
    "for i in range(optimal_n_clusters_reduced):\n",
    "    print(f\"Cluster {i} titles:\")\n",
    "    print(df_valid_articles[df_valid_articles['cluster'] == i]['title'].head(10), \"\\n\")  # Displaying the first 10 titles per cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Matching Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text):\n",
    "    tokens = simple_preprocess(text, deacc=True) # remove punctuation, see below for notes\n",
    "    stopwords_list = set(stopwords.words('english'))\n",
    "    keywords = [word for word in tokens if word not in stopwords_list]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_qns['keywords'] = train_qns['question'].apply(extract_keywords)\n",
    "train_qns[['question', 'keywords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['keywords'] = articles['text'].apply(extract_keywords)\n",
    "articles[['title', 'keywords']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's create a merged dataset for easier visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = train_qns.merge(articles, left_on='article', right_on='title')\n",
    "merged_data['question_keywords'] = merged_data['keywords_x']\n",
    "merged_data['article_keywords']= merged_data['keywords_y']\n",
    "merged_data['question_keywords_str'] = merged_data['question_keywords'].apply(lambda x: ' '.join(x))\n",
    "merged_data['article_keywords_str'] = merged_data['article_keywords'].apply(lambda x: ' '.join(x))\n",
    "merged_data = merged_data.drop(['keywords_x', 'keywords_y'], axis=1)\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalize_and_lemmatize(keyword_list):\n",
    "    normalized = [re.sub(r'[^a-zA-Z]', '', word.lower()) for word in keyword_list]\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in normalized if word]\n",
    "    return lemmatized\n",
    "\n",
    "keyword_matching_dataset = merged_data.copy()\n",
    "keyword_matching_dataset['question_keywords'] = keyword_matching_dataset['question_keywords'].apply(normalize_and_lemmatize)\n",
    "keyword_matching_dataset['article_keywords'] = keyword_matching_dataset['article_keywords'].apply(normalize_and_lemmatize)\n",
    "\n",
    "keyword_matching_dataset['question_keywords_str'] = keyword_matching_dataset['question_keywords'].apply(lambda x: ' '.join(x))\n",
    "keyword_matching_dataset['article_keywords_str'] = keyword_matching_dataset['article_keywords'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_matching_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TI-IDF\n",
    "\n",
    "use of cosine similarity\n",
    "\n",
    "Measure of Orientation, Not Magnitude: Cosine similarity focuses on the angle between two vectors, not their magnitude. This means it can effectively determine how similar two documents are in terms of their content, regardless of their length.\n",
    "\n",
    "Robust to Different Lengths: Since it's based on vector orientation, cosine similarity is less affected by the length of documents. This is particularly useful when comparing texts of different lengths.\n",
    "\n",
    "Intuitive Interpretation: The cosine similarity score ranges from -1 to 1, where 1 indicates identical directionality (and thus, high similarity), 0 indicates no similarity, and -1 indicates completely opposite. This makes the results relatively straightforward to interpret.\n",
    "\n",
    "Effectiveness in High-Dimensional Space: Text data, especially when vectorized using methods like TF-IDF, often exists in a high-dimensional space. Cosine similarity is effective in such spaces, where Euclidean distance can become less meaningful due to the curse of dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix_questions = tfidf_vectorizer.fit_transform(keyword_matching_dataset['question_keywords_str'])\n",
    "tfidf_matrix_articles = tfidf_vectorizer.transform(keyword_matching_dataset['article_keywords_str'])\n",
    "\n",
    "# calculate cosine similarity between each question and its corresponding article\n",
    "cosine_sim_tfidf = [cosine_similarity(tfidf_matrix_questions[i], tfidf_matrix_articles[i])[0][0] for i in range(len(keyword_matching_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize TF-IDF Vectorizer with n-grams (e.g., unigrams and bigrams)\n",
    "tfidf_vectorizer_ngrams = TfidfVectorizer(ngram_range=(1, 2))\n",
    "tfidf_matrix_questions_ngrams = tfidf_vectorizer_ngrams.fit_transform(keyword_matching_dataset['question_keywords_str'])\n",
    "tfidf_matrix_articles_ngrams = tfidf_vectorizer_ngrams.transform(keyword_matching_dataset['article_keywords_str'])\n",
    "\n",
    "# calculate cosine similarity between each question and its corresponding article\n",
    "cosine_sim_ngrams = [cosine_similarity(tfidf_matrix_questions_ngrams[i], tfidf_matrix_articles_ngrams[i])[0][0] for i in range(len(keyword_matching_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bm25_for_pair(query, article_text, bm25_model):\n",
    "    query_tokens = query.split(\" \")\n",
    "    article_tokens = article_text.split(\" \")\n",
    "    doc_scores = bm25_model.get_scores(query_tokens)\n",
    "    article_index = tokenized_corpus.index(article_tokens)\n",
    "    return doc_scores[article_index]\n",
    "\n",
    "tokenized_corpus = [doc.split(\" \") for doc in merged_data['article_keywords_str']]\n",
    "bm25_model = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed # for lazy execution, sets up task graph\n",
    "def delayed_calculate_bm25_for_pair(query, article_text, bm25_model): # delayed task\n",
    "    return calculate_bm25_for_pair(query, article_text, bm25_model)\n",
    "\n",
    "dask_df = dd.from_pandas(merged_data, npartitions=4) # adjust partitions \n",
    "\n",
    "bm25_scores = [delayed_calculate_bm25_for_pair(row['question_keywords_str'], row['article_keywords_str'], bm25_model) for index, row in dask_df.iterrows()] # series of independent task for each row\n",
    "\n",
    "bm25_results = dd.compute(*bm25_scores) # executes the parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_matching_dataset['cosine_sim_tfidf'] = cosine_sim_tfidf\n",
    "keyword_matching_dataset['cosine_sim_ngrams'] = cosine_sim_ngrams\n",
    "keyword_matching_dataset['bm25_score'] = bm25_results\n",
    "\n",
    "keyword_matching_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_score_reshaped = np.array(keyword_matching_dataset['bm25_score']).reshape(-1, 1)\n",
    "scaler = MinMaxScaler()\n",
    "keyword_matching_dataset['norm_bm25_score'] = scaler.fit_transform(bm25_score_reshaped)\n",
    "\n",
    "keyword_matching_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sim_tfidf = np.mean(keyword_matching_dataset['cosine_sim_tfidf'])\n",
    "mean_sim_ngrams = np.mean(keyword_matching_dataset['cosine_sim_ngrams'])\n",
    "mean_norm_bm25_score = np.mean(keyword_matching_dataset['norm_bm25_score'])\n",
    "\n",
    "print(f\"Mean Cosine Similarity (TF-IDF): {mean_sim_tfidf}\")\n",
    "print(f\"Mean Cosine Similarity (N-grams): {mean_sim_ngrams}\")\n",
    "print(f\"Mean BM25 Score: {mean_norm_bm25_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seems like both are performing poorly. we can look at this distribution further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(keyword_matching_dataset['points'], keyword_matching_dataset['cosine_sim_tfidf'], label='TF-IDF', alpha=0.5)\n",
    "plt.scatter(keyword_matching_dataset['points'], keyword_matching_dataset['cosine_sim_ngrams'], label='N-grams', alpha=0.5)\n",
    "plt.scatter(keyword_matching_dataset['points'], keyword_matching_dataset['norm_bm25_score'], label='BM25', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Points')\n",
    "plt.ylabel('Cosine Similarity')\n",
    "plt.title('Correlation between Points and Cosine Similarity')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no clear pattern, points scatter across almost all possible values. ti-idf outperforms n-grams by a marginal extent and bm25 to a greater extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_similarity_threshold = 0.8\n",
    "low_similarity_threshold = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-points, high-similarity outliers for TF-IDF\n",
    "high_points_high_tfidf = keyword_matching_dataset[\n",
    "    (keyword_matching_dataset['points'] > keyword_matching_dataset['points'].quantile(0.9)) & \n",
    "    (keyword_matching_dataset['cosine_sim_tfidf'] > high_similarity_threshold)\n",
    "]\n",
    "\n",
    "# High-points, low-similarity outliers for TF-IDF\n",
    "high_points_low_tfidf = keyword_matching_dataset[\n",
    "    (keyword_matching_dataset['points'] > keyword_matching_dataset['points'].quantile(0.9)) & \n",
    "    (keyword_matching_dataset['cosine_sim_tfidf'] < low_similarity_threshold)\n",
    "]\n",
    "\n",
    "# High-points, high-similarity outliers for N-grams\n",
    "high_points_high_ngrams = keyword_matching_dataset[\n",
    "    (keyword_matching_dataset['points'] > keyword_matching_dataset['points'].quantile(0.9)) & \n",
    "    (keyword_matching_dataset['cosine_sim_ngrams'] > high_similarity_threshold)\n",
    "]\n",
    "\n",
    "# High-points, low-similarity outliers for N-grams\n",
    "high_points_low_ngrams = keyword_matching_dataset[\n",
    "    (keyword_matching_dataset['points'] > keyword_matching_dataset['points'].quantile(0.9)) & \n",
    "    (keyword_matching_dataset['cosine_sim_ngrams'] < low_similarity_threshold)\n",
    "]\n",
    "\n",
    "# High-points, high-similarity outliers for BM25\n",
    "high_points_high_bm25 = keyword_matching_dataset[\n",
    "    (keyword_matching_dataset['points'] > keyword_matching_dataset['points'].quantile(0.9)) & \n",
    "    (keyword_matching_dataset['bm25_score'] > high_similarity_threshold)\n",
    "]\n",
    "\n",
    "# High-points, low-similarity outliers for BM25\n",
    "\n",
    "high_points_low_bm25 = keyword_matching_dataset[\n",
    "    (keyword_matching_dataset['points'] > keyword_matching_dataset['points'].quantile(0.9)) & \n",
    "    (keyword_matching_dataset['bm25_score'] < low_similarity_threshold)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_points_high_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_points_low_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as suspected, some questions with very high points have answers in a very specific part of an article which would not have the same keywords as the question (**rephrase**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_points_high_ngrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_points_low_ngrams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exactly the same performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_points_high_bm25.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_points_low_bm25.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same performance as other two models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we should move onto models that capture the semantic content behind the words\n",
    "\n",
    "embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_matching_dataset = merged_data.copy()\n",
    "\n",
    "semantic_matching_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md', disable=['parser', 'tagger', 'ner'])\n",
    "\n",
    "def get_spacy_vectors(texts, batch_size=100):\n",
    "    vectors = []\n",
    "    for doc in nlp.pipe(texts, batch_size=batch_size):\n",
    "        vectors.append(doc.vector)\n",
    "    return np.array(vectors)\n",
    "\n",
    "questions = semantic_matching_dataset['question'].tolist()\n",
    "articles = semantic_matching_dataset['text'].tolist()\n",
    "\n",
    "question_vectors_spacy = get_spacy_vectors(questions)\n",
    "article_vectors_spacy = get_spacy_vectors(articles)\n",
    "cosine_similarities_spacy = cosine_similarity(question_vectors_spacy, article_vectors_spacy)\n",
    "cosine_sim_scores = np.diag(cosine_similarities_spacy)\n",
    "semantic_matching_dataset['cosine_sim_spacy'] = cosine_sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "def encode_texts_in_batches(texts, model, batch_size=128):\n",
    "    vectors = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        # ensure the model's output is converted to tensors and moved to CPU before converting to numpy\n",
    "        batch_vectors = model.encode(batch, convert_to_tensor=True).cpu().numpy()\n",
    "        vectors.extend(batch_vectors)\n",
    "    return np.array(vectors)\n",
    "\n",
    "question_vectors_sbert = encode_texts_in_batches(\n",
    "    semantic_matching_dataset['question'].tolist(), sbert_model)\n",
    "article_vectors_sbert = encode_texts_in_batches(\n",
    "    semantic_matching_dataset['text'].tolist(), sbert_model)\n",
    "\n",
    "cosine_sim_scores = [cosine_similarity([q_vec], [a_vec])[0][0] for q_vec, a_vec in zip(question_vectors_sbert, article_vectors_sbert)]\n",
    "semantic_matching_dataset['cosine_sim_sbert'] = cosine_sim_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may need to delete cached models before running use model\n",
    "# !rm -rf /var/folders/j3/2xy_ffxd6jq618phh49pk1b00000gn/T/tfhub_modules/*\n",
    "\n",
    "use_model = hub.load('https://tfhub.dev/google/universal-sentence-encoder/4')\n",
    "\n",
    "\n",
    "def batch_get_use_vector(texts, batch_size=32):\n",
    "    vectors = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_vectors = use_model(batch_texts).numpy()\n",
    "        vectors.append(batch_vectors)\n",
    "    return np.vstack(vectors)\n",
    "\n",
    "questions = semantic_matching_dataset['question'].tolist()\n",
    "articles = semantic_matching_dataset['text'].tolist()\n",
    "\n",
    "semantic_matching_dataset['question_vector_use'] = list(map(list, batch_get_use_vector(questions)))\n",
    "semantic_matching_dataset['article_vector_use'] = list(map(list, batch_get_use_vector(articles)))\n",
    "\n",
    "cosine_sims = np.array([\n",
    "    cosine_similarity([q_vec], [a_vec])[0][0] \n",
    "    for q_vec, a_vec in zip(semantic_matching_dataset['question_vector_use'], semantic_matching_dataset['article_vector_use'])\n",
    "])\n",
    "\n",
    "semantic_matching_dataset['cosine_sim_use'] = cosine_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_matching_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sim_spacy = np.mean(semantic_matching_dataset['cosine_sim_spacy'])\n",
    "mean_sim_sbert = np.mean(semantic_matching_dataset['cosine_sim_sbert'])\n",
    "mean_sim_use = np.mean(semantic_matching_dataset['cosine_sim_use'])\n",
    "\n",
    "print(f\"Mean Cosine Similarity (SpaCy): {mean_sim_spacy}\")\n",
    "print(f\"Mean Cosine Similarity (SBERT): {mean_sim_sbert}\")\n",
    "print(f\"Mean Cosine Similarity (USE): {mean_sim_use}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy embedding model clearly outperforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(semantic_matching_dataset['points'], semantic_matching_dataset['cosine_sim_spacy'], label='SpaCy', alpha=0.5)\n",
    "plt.scatter(semantic_matching_dataset['points'], semantic_matching_dataset['cosine_sim_sbert'], label='SBERT', alpha=0.5)\n",
    "plt.scatter(semantic_matching_dataset['points'], semantic_matching_dataset['cosine_sim_use'], label='USE', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Points')\n",
    "plt.ylabel('Cosine Similarity')\n",
    "plt.title('Correlation between Points and Cosine Similarity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seems that spacy model captures a good portion of the lower scored questions. let's explore the question-article pairs that scored a near perfect 1.00 similarity score, as well as the opposite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_matching_dataset[semantic_matching_dataset['cosine_sim_spacy'] >= 0.9]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direct Question-Answer Relation: Each question directly relates to the topic of the article. The keywords from the questions are explicitly present in the text of the articles, leading to a higher similarity score.\n",
    "\n",
    "Specificity: The questions are specific, and the articles contain detailed information that directly answers these questions. This specificity likely results in a higher concentration of relevant terms and less noise, which boosts similarity scores.\n",
    "\n",
    "Relevance of Content: The content of the questions closely aligns with the main subject of the articles. For example, a question about the environment directly relates to an article titled \"Environment,\" and a question about MRI accuracy directly relates to an article about \"Magnetic resonance imaging.\"\n",
    "\n",
    "High readability scores as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_matching_dataset[semantic_matching_dataset['cosine_sim_spacy'] <= 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mismatch of Context: The questions are about specific events or people, while the articles titled with years likely cover a wide range of events that happened in that year. The spaCy embeddings might not find a strong contextual match between the question's focus and the broad content of the article.\n",
    "\n",
    "Lack of Specificity in Articles: Articles that are simply titled with a year might not have a strong thematic focus, leading to a diluted set of vectors that don't match well with the more focused vectors generated from the questions.\n",
    "\n",
    "Semantic Ambiguity: Years by themselves do not carry specific semantic information. Without additional context, the model may struggle to link a question about a specific event or individual to an article that broadly covers everything related to that year.\n",
    "\n",
    "Named Entity Recognition (NER) Challenges: If the model is heavily weighing named entities, such as specific names and dates, it may not correctly associate the relevance of the year to the specific event or individual in the question.\n",
    "\n",
    "Temporal Relevance: Questions that ask about a particular date or event may not align well with the content of an article that summarizes an entire year. The temporal focus is too broad in the article to match the specificity of the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given that we are not able to modify the training and test data set, fine tuning the data for the spacy model would hence be quite challenging. we could instead look for alternative models that are specifically designed for chatbots (most common use case of question-answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory_usage():\n",
    "    available_memory = psutil.virtual_memory().available\n",
    "    total_memory = psutil.virtual_memory().total\n",
    "    used_memory = total_memory - available_memory\n",
    "    print(f\"Used memory: {used_memory / (1024**3):.2f} GB\")\n",
    "    print(f\"Available memory: {available_memory / (1024**3):.2f} GB\")\n",
    "    print(f\"Total memory: {total_memory / (1024**3):.2f} GB\")\n",
    "\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del nlp, use_model, sbert_model\n",
    "del question_vectors_spacy, article_vectors_spacy\n",
    "del question_vectors_sbert, article_vectors_sbert\n",
    "del tfidf_matrix_questions, tfidf_matrix_articles, cosine_sims, cosine_sim_scores\n",
    "\n",
    "columns_to_drop = ['question_vector_use', 'article_vector_use', 'question_vector_spacy', 'article_vector_spacy', 'question_vectors_sbert', 'article_vectors_sbert', 'cosine_sim_use', 'cosine_sim_spacy', 'cosine_sim_sbert']\n",
    "semantic_matching_dataset = semantic_matching_dataset.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "gc.collect()\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "def encode_documents(documents):\n",
    "    print('Encoding documents...')\n",
    "    encoded_input = tokenizer(documents, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    return embeddings.cpu().detach().numpy()\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    print('Mean pooling...')\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "corpus = semantic_matching_dataset['text'].values.tolist()\n",
    "corpus_embeddings = encode_documents(corpus)\n",
    "\n",
    "index = faiss.IndexFlatL2(corpus_embeddings.shape[1])\n",
    "index.add(corpus_embeddings)\n",
    "\n",
    "\n",
    "def search(query, k=5):\n",
    "    query_embedding = transformer_model.encode(query)\n",
    "    distances, indices = index.search(np.array([query_embedding]), k)\n",
    "    return [(corpus[i], distances[0][j]) for j, i in enumerate(indices[0])]\n",
    "\n",
    "query = semantic_matching_dataset['question']\n",
    "relevant_docs = search(query)\n",
    "print(relevant_docs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
